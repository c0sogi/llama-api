version: '3'

services:
  llama-api:
    build:
      context: .
      dockerfile: Dockerfile
    entrypoint: ["python3", "-m", "main", "--port", "8000"]
    environment:
      - MAX_WORKERS=1
    volumes:
      - ./models:/app/models
      - ./llama_api:/app/llama_api
      - ./model_definitions.py:/app/model_definitions.py
      - ./main.py:/app/main.py
      - ./requirements.txt:/app/requirements.txt
    ports:
      - 8000:8000
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            capabilities: [gpu]