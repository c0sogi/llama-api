version: '3'

services:
  llama-api:
    image: cosogi/llama-api:latest
    cap_add:
      - IPC_LOCK
    entrypoint: ["python3", "-m", "main", "--port", "8000"]
    environment:
      - FORCE_CUDA=1
      - LLAMA_API_MAX_WORKERS=1
      - LLAMA_API_API_KEY=
    volumes:
      - ./models:/app/models
      - ./llama_api:/app/llama_api
      - ./model_definitions.py:/app/model_definitions.py
      - ./main.py:/app/main.py
      - ./requirements.txt:/app/requirements.txt
      - ./pyproject.toml:/app/pyproject.toml
    ports:
      - 8000:8000
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            capabilities: [gpu]