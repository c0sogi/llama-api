version: '3'

services:
  llama-api:
    image: cosogi/llama-api:230730
    entrypoint: ["python3", "-m", "main", "--port", "8000"]
    environment:
      - MAX_WORKERS=1
    volumes:
      - ./models:/app/models
      - ./llama_api:/app/llama_api
      - ./model_definitions.py:/app/model_definitions.py
      - ./main.py:/app/main.py
      - ./requirements.txt:/app/requirements.txt
      - ./pyproject.toml:/app/pyproject.toml
    ports:
      - 8000:8000
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            capabilities: [gpu]

# services:
  # llama-api:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile
  #   entrypoint: ["python3", "-m", "main", "--port", "8000"]
  #   environment:
  #     - MAX_WORKERS=1
  #   volumes:
  #     - ./models:/app/models
  #     - ./llama_api:/app/llama_api
  #     - ./model_definitions.py:/app/model_definitions.py
  #     - ./main.py:/app/main.py
  #     - ./requirements.txt:/app/requirements.txt
  #   ports:
  #     - 8000:8000
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #         - driver: nvidia
  #           capabilities: [gpu]