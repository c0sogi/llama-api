version: '3.8'

volumes:
  llama-api-models:

services:
  llama-api:
    image: cosogi/llama-api:230814
    entrypoint: ["python3", "-m", "main", "--port", "8000"]
    environment:
      - FORCE_CUDA=1
      - LLAMA_API_MAX_WORKERS=1
      - LLAMA_API_API_KEY=
    volumes:
      - llama-api-models:/app/models
      - ./model_definitions.py:/app/model_definitions.py
      - ./main.py:/app/main.py
    ports:
      - 8000:8000
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            capabilities: [gpu]


# services:
#   llama-api:
#     build:
#       context: .
#       dockerfile: Dockerfile
#     entrypoint: ["python3", "-m", "main", "--port", "8000"]
#     environment:
#       - LLAMA_API_MAX_WORKERS=1
#       - LLAMA_API_API_KEY=
#     volumes:
#       - llama-api-models:/app/models
#       - ./model_definitions.py:/app/model_definitions.py
#       - ./main.py:/app/main.py
#     ports:
#       - 8000:8000
#     deploy:
#       resources:
#         reservations:
#           devices:
#           - driver: nvidia
#             capabilities: [gpu]