[tool.poetry]
name = "llama-api"
version = "0.1.1"
description = "An OpenAI-like LLaMA inference API"
authors = ["c0sogi <cosogi@icloud.com>"]
license = "MIT"
readme = "readme.md"
homepage = "https://github.com/c0sogi/llama-api"
repository = "https://github.com/c0sogi/llama-api"
packages = [{ include = "llama_api" }]
include = ["LICENSE.md"]

[tool.poetry.dependencies]
python = ">=3.8.1,<3.12"
poetry = "^1.5.1"

uvicorn = { extras = ["standard"], version = "^0.23" }
fastapi = ">=0.100.1"
orjson = "^3.9"
sse-starlette = "^1.6"
psutil = "^5.9"
cmake = ">=3.18.0"
filelock = "^3.12"
transformers = "^4.31.0"
tensorflow-hub = ">=0.14"
numpy = "^1.24.3"
safetensors = "^0.3.1"
ninja = "^1.11.1"
diskcache = "^5.6.1"
pydantic = "^2.0.0"
pydantic-settings = "^2.0.0"
sentencepiece = ">=0.1.97"
typing-extensions = ">=4.0.0"
tiktoken = ">=0.4.0"
# torch: 2.0.1+cu118 for GPU, 2.0.1+cpu for CPU

[tool.poetry.group.dev.dependencies]
black = "^23.7.0"
twine = "^4.0.2"
flake8 = "^6.0.0"
mkdocs = "^1.4.3"
mkdocstrings = { extras = ["python"], version = "^0.22.0" }
mkdocs-material = "^9.1.19"
pytest = "^7.4.0"
pytest-asyncio = "^0.21.1"
httpx = "^0.24.1"

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"
